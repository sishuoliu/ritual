# 通用 RPG 桌游引擎可行性分析 — 批判性审查

> 对《通用 RPG 桌游引擎_可行性分析.md》的批判性审视  
> 日期：2026-01-29

---

## 一、核心假设的批判

### 1.1 "80%+ 游戏可覆盖" 的质疑

**原文档声称**：
> 核心层：阶段、行动、资源、牌堆、胜负条件、分支选择 —— 足以覆盖 80%+ 游戏。

**批判**：
- ❌ **缺乏数据支撑**：这个 80% 数字从何而来？是基于 BGG 统计、还是主观估计？
- ❌ **"覆盖"的定义模糊**：是指「能跑起来」还是「能完整表达所有机制」？例如：
  - Gloomhaven 的卡牌驱动战斗系统：手牌管理、卡牌组合、行动选择、卡牌消耗/恢复 —— 这些细节能否用 YAML 简洁表达？
  - Twilight Imperium 的 30 个非对称阵营：每个阵营有独特能力、科技树、起始资源、胜利路径 —— 配置量会爆炸式增长
- ⚠️ **"覆盖" ≠ "好用"**：即使能表达，配置文件的复杂度可能让设计师望而却步

**建议**：
- 明确「覆盖」的定义：是「能模拟」还是「能完整表达所有机制」
- 用实际案例验证：选 3-5 款不同复杂度的游戏，尝试用 YAML 完整建模，评估配置量和工作量
- 承认「长尾问题」：20% 的游戏可能需要特殊处理或无法支持

---

### 1.2 YAML 配置表达力的局限性

**原文档假设**：
> 用 YAML/JSON 配置即可表达绝大多数 RPG 桌游机制

**批判**：
- ❌ **条件逻辑的复杂性**：许多游戏有复杂的条件判断，例如：
  ```yaml
  # 原文档示例
  trigger: { round: 3, phase: start, condition: "player.refuge == true" }
  ```
  但实际游戏中可能有：
  - 「如果玩家 A 的福 > 5 且玩家 B 的慧 < 3，且当前劫难 > 10，则触发事件 X」
  - 「当任意玩家完成第 3 次渡化，且该玩家是皈依者，且游戏已进行超过 4 回合，则...」
  
  这些复杂条件在 YAML 中会变成：
  ```yaml
  condition: "player_a.merit > 5 AND player_b.wisdom < 3 AND global.calamity > 10"
  ```
  这本质上是在 YAML 里嵌入一门脚本语言，失去了「配置」的简洁性。

- ❌ **动态行为的表达**：某些游戏机制是「过程式」而非「声明式」：
  - Gloomhaven 的卡牌组合：玩家需要「思考」如何组合手牌，这不是规则能直接表达的
  - 谈判类游戏（如 Diplomacy）：玩家间的协议、背叛、联盟 —— 这些是「社交行为」，难以用配置表达

- ⚠️ **配置文件的爆炸式增长**：以 Twilight Imperium 为例：
  - 30 个阵营 × 每个阵营 10+ 独特能力 = 300+ 条配置
  - 科技树、行动卡、事件卡、目标卡 = 数百条配置
  - 最终配置文件可能达到数千行，难以维护

**建议**：
- 承认 YAML 的局限性，设计「脚本扩展」机制：复杂逻辑用 Python 脚本表达
- 提供「配置模板」和「代码生成器」：LLM 生成配置 → 引擎转换为代码 → 执行
- 明确「支持范围」：哪些机制用配置，哪些需要代码

---

### 1.3 LLM 生成 YAML 的可靠性被高估

**原文档声称**：
> DeepSeek API 已支持代码生成和 JSON 结构化输出；通过 Prompt 工程 + 校验循环，可实现「自然语言 → YAML 规则」的可靠转换。

**批判**：
- ❌ **LLM 的「幻觉」问题**：LLM 可能生成：
  - 语法正确但语义错误的配置（例如：`cost: { wisdom: -5 }` 负数成本）
  - 引用了不存在的资源/行动（例如：`requires: { player.karma: ">=10" }` 但 karma 未定义）
  - 逻辑矛盾（例如：同时要求 `merit >= 5` 和 `merit < 3`）

- ❌ **校验循环的成本**：
  - 如果 LLM 生成的配置有错误，需要：
    1. 引擎检测错误
    2. 将错误信息反馈给 LLM
    3. LLM 修正
    4. 再次校验
  - 这个过程可能需要 3-5 轮，每次调用 API 都有延迟和成本
  - 如果 LLM 无法理解错误，可能陷入死循环

- ⚠️ **上下文窗口限制**：
  - 复杂游戏的配置可能达到数千行
  - LLM 的上下文窗口有限（DeepSeek 约 32k tokens）
  - 如果用户要修改大型配置，LLM 可能无法看到完整上下文

**建议**：
- 设计「增量修改」机制：只让 LLM 生成/修改配置片段，而非整个文件
- 提供「配置验证器」：在 LLM 生成后立即校验，给出明确的错误提示
- 设置「修正次数上限」：避免死循环
- 承认「LLM 辅助」而非「LLM 全自动」：设计师仍需审查和调整

---

## 二、技术难点的低估

### 2.1 隐藏信息模拟的复杂性

**原文档评估**：
> ⚠️ 中：需引擎支持「视角」（哪些信息对哪些玩家可见）；模拟时需决定是「全知」还是「有限信息」

**批判**：
- ❌ **严重低估**：隐藏信息模拟是**极高难度**的问题：
  - **状态空间爆炸**：如果玩家 A 不知道玩家 B 的手牌，AI 需要维护「信念状态」（belief state）：玩家 B 可能有哪些手牌的概率分布
  - **多智能体推理**：玩家 A 需要推理「玩家 B 可能知道什么」「玩家 B 可能如何行动」
  - **叛徒机制**：在 Dead of Winter 中，叛徒需要「假装合作」但「暗中破坏」—— 这需要 AI 具备「欺骗」和「伪装」能力，远超当前 RL 的能力范围

- ❌ **「全知」模拟的局限性**：
  - 如果所有 AI 都是「全知」，它们会做出「最优但非人类」的决策
  - 例如：在 Hanabi 中，全知 AI 会完美配合，但真人玩家无法做到
  - 这导致模拟结果与真人体验偏差巨大

**建议**：
- 明确「隐藏信息」是 Phase 5+ 的高级功能，而非 MVP
- 先做「完全信息」版本，用于测试规则一致性和数值平衡
- 隐藏信息模拟需要专门的 AI 研究（Theory of Mind、Belief State、Multi-Agent RL）

---

### 2.2 多智能体合作的策略模拟

**原文档评估**：
> ⚠️ 中：隐藏信息 + 多人合作/叛徒的策略模拟难度高；可先做「完全信息」版本

**批判**：
- ❌ **「可先做完全信息版本」过于简化**：
  - 即使完全信息，多人合作仍有挑战：
    - **协调问题**：4 个 AI 如何协调行动？是「中央规划」还是「分布式决策」？
    - **策略多样性**：如果所有 AI 都用同一策略，会陷入「局部最优」
    - **叛徒模拟**：在半合作游戏中，叛徒需要「伪装」—— 即使完全信息，AI 如何模拟「假装合作但暗中破坏」？

- ❌ **RL 训练的挑战**：
  - 多人合作 RL 的状态空间巨大：4 玩家 × 每玩家 N 种行动 = N^4 组合
  - 训练时间长：可能需要数天到数周
  - 策略可能不稳定：训练出的策略可能在特定场景下失效

**建议**：
- MVP 阶段只用「简单策略」（随机、贪心、规则型），不做 RL
- RL 作为 Phase 5+ 的高级功能
- 明确「模拟目标」：是「发现规则漏洞」还是「预测真人体验」？

---

### 2.3 规则一致性与边界情况

**原文档未充分讨论**：

**批判**：
- ❌ **规则冲突检测**：
  - 如果用户通过 LLM 添加新规则，可能与现有规则冲突
  - 例如：添加「每回合财富 +1」，但已有「每回合财富 -1」—— 引擎如何检测？
  - 更复杂的冲突：两个规则在不同条件下生效，但组合后产生矛盾

- ❌ **边界情况处理**：
  - 「资源为负怎么办？」「行动成本超过资源上限怎么办？」「同时触发多个事件怎么办？」
  - 这些边界情况需要在引擎层面处理，但 YAML 配置可能无法完整表达

- ❌ **规则优先级**：
  - 如果多个规则同时生效，优先级如何定义？
  - 例如：「渡化时 +1 福」和「渡化时 +2 福」同时存在，是叠加还是覆盖？

**建议**：
- 设计「规则冲突检测器」：在加载配置时检查冲突
- 定义「规则优先级系统」：明确叠加、覆盖、替换的规则
- 提供「边界情况处理模板」：常见边界情况的处理方式

---

## 三、实际开发中的问题

### 3.1 配置文件的维护成本

**原文档未充分讨论**：

**批判**：
- ❌ **配置文件会变得巨大**：
  - 以 Gloomhaven 为例：17 个角色 × 每角色 20+ 卡牌 × 每卡牌多行配置 = 数百行
  - 加上事件卡、怪物卡、场景配置 = 数千行
  - 维护这样的配置文件是噩梦

- ❌ **版本管理困难**：
  - 如果用户通过 LLM 多次修改配置，如何追踪变更？
  - 如何回滚到之前的版本？
  - 如何合并多个设计师的修改？

- ❌ **调试困难**：
  - 如果游戏运行出错，如何定位是配置的哪一行？
  - YAML 的错误信息可能不够友好

**建议**：
- 提供「配置模块化」：将配置拆分为多个文件（角色.yaml、事件.yaml、行动.yaml）
- 提供「配置版本控制」：集成 Git 或自建版本系统
- 提供「配置调试工具」：可视化配置依赖关系、检测循环引用

---

### 3.2 不同游戏之间的差异

**原文档假设**：
> 通过分层抽象，用 YAML 配置即可表达绝大多数 RPG 桌游机制

**批判**：
- ❌ **游戏之间的差异可能比相似性更大**：
  - **Gloomhaven**：卡牌驱动、无骰子、手牌管理
  - **Twilight Imperium**：区域控制、外交、科技树、非对称阵营
  - **Dead of Winter**：隐藏目标、投票、资源管理、危机
  - **Betrayal**：探索、Haunt 触发、剧本系统

  这些游戏的「核心循环」完全不同，强行用同一套抽象可能导致：
  - 配置复杂度过高
  - 某些机制「削足适履」
  - 引擎变得臃肿

- ⚠️ **「通用引擎」的陷阱**：
  - 试图支持所有游戏 → 引擎变得复杂 → 难以维护
  - 更好的策略可能是：核心引擎 + 游戏特定扩展

**建议**：
- 明确「支持范围」：先支持 2-3 类相似游戏（如半合作类），再逐步扩展
- 设计「插件系统」：游戏特定机制通过插件实现
- 承认「不是所有游戏都适合」：某些游戏可能需要专门工具

---

### 3.3 学习曲线与用户体验

**原文档未充分讨论**：

**批判**：
- ❌ **YAML 配置的学习曲线**：
  - 非程序员设计师可能不熟悉 YAML 语法
  - 即使有 LLM 辅助，用户仍需理解生成的配置
  - 错误配置可能导致游戏无法运行，用户需要调试能力

- ❌ **LLM 对话的局限性**：
  - 用户可能不知道如何「提问」才能得到想要的规则
  - LLM 可能误解用户意图，生成不符合预期的配置
  - 多轮对话可能让用户感到「沟通困难」

- ⚠️ **可视化工具的缺失**：
  - 原文档提到「可视化工具」，但未详细设计
  - 如果用户只能看 YAML 文本，体验会很差
  - 需要「可视化编辑器」：拖拽式界面、实时预览

**建议**：
- 提供「图形化配置编辑器」：类似 Unity Inspector，而非纯文本
- 提供「规则模板库」：常见机制的模板，用户可直接使用
- 提供「交互式教程」：引导用户学习配置语法

---

## 四、商业与市场风险

### 4.1 目标用户不明确

**原文档未充分讨论**：

**批判**：
- ❌ **谁是目标用户？**
  - **独立设计师**：可能更愿意用 Tabletop Simulator + 手写规则
  - **专业工作室**：可能已有内部工具，不愿切换
  - **业余爱好者**：可能不需要如此复杂的工具

- ❌ **用户痛点是否真实？**
  - 「快速迭代」是痛点吗？还是「平衡性测试」？
  - 用户是否愿意学习新工具？
  - 是否有足够的用户基数支撑开发？

**建议**：
- 进行「用户调研」：访谈 10-20 位桌游设计师，了解真实痛点
- 明确「MVP 目标用户」：先服务一个细分群体（如「想做半合作 RPG 的独立设计师」）
- 设计「用户旅程」：从「听说工具」到「成功设计游戏」的完整路径

---

### 4.2 竞争与差异化

**原文档未充分讨论**：

**批判**：
- ❌ **现有工具的优势**：
  - **Tabletop Simulator**：已有大量用户、支持多人联机、可视化好
  - **Ludii**：通用游戏系统、已有 100+ 游戏、开源
  - **VASSAL**：成熟、稳定、社区大

- ❌ **本项目的差异化在哪里？**
  - 「LLM 辅助设计」是差异化，但可能不够
  - 「AI 回测」是差异化，但 SIMPLE 等已有类似功能
  - 需要更明确的「价值主张」

**建议**：
- 明确「核心差异化」：是「LLM 对话式设计」还是「AI 回测」？
- 设计「不可替代性」：用户为什么必须用这个工具？
- 考虑「开源策略」：开源核心引擎，吸引社区贡献

---

## 五、修正后的可行性评估

### 5.1 重新评估核心问题

| 问题 | 原评估 | 修正后评估 | 说明 |
|------|--------|------------|------|
| **能否做一套引擎支持所有 RPG 桌游？** | ✅ 可以（80%+） | ⚠️ **部分可以**（50-60%） | 需要明确「支持」的定义；极复杂游戏配置量过大 |
| **LLM 对话式设计可行吗？** | ✅ 可行 | ⚠️ **部分可行** | LLM 生成需要多轮校验；复杂配置可能超出上下文窗口 |
| **模糊/精确需求都能处理吗？** | ✅ 可以 | ⚠️ **模糊需求困难** | 模糊需求需要多轮对话，用户体验可能不佳 |
| **功德轮回的分支选择能实现吗？** | ✅ 完全可以 | ✅ **可以** | 这是相对简单的机制，确实可行 |

### 5.2 修正后的风险清单

| 风险 | 原评估 | 修正后评估 | 缓解措施 |
|------|--------|------------|----------|
| **LLM 生成错误** | 中（校验循环） | ⚠️ **高** | 增量修改、验证器、修正上限 |
| **配置文件爆炸** | 未充分讨论 | ⚠️ **高** | 模块化、代码生成、明确支持范围 |
| **隐藏信息模拟** | 中（先做完全信息） | ⚠️ **极高** | Phase 5+，需要专门研究 |
| **多智能体合作** | 中（RL 训练） | ⚠️ **高** | MVP 只用简单策略，RL 作为高级功能 |
| **用户学习曲线** | 未充分讨论 | ⚠️ **中高** | 图形化编辑器、模板库、教程 |
| **目标用户不明确** | 未充分讨论 | ⚠️ **中** | 用户调研、明确细分市场 |

---

## 六、修正后的实施建议

### 6.1 缩小 MVP 范围

**原文档建议**：
> Phase 1：核心引擎 + 功德轮回 v3.6 配置化 + 随机模拟

**修正建议**：
- ✅ **保留**：核心引擎 + 功德轮回 v3.6
- ❌ **移除**：不要试图支持「所有 RPG 桌游」
- ✅ **明确**：MVP 只支持「半合作 RPG」类游戏（功德轮回、Dead of Winter 等）
- ✅ **限制**：只支持「完全信息」版本，隐藏信息作为 Phase 5+

### 6.2 重新设计 LLM 集成

**原文档假设**：
> LLM 可以直接生成完整 YAML 配置

**修正建议**：
- ⚠️ **增量修改**：LLM 只生成/修改配置片段，而非整个文件
- ⚠️ **模板驱动**：提供常见机制的模板，LLM 基于模板生成
- ⚠️ **多轮确认**：每次修改后要求用户确认，避免误解
- ⚠️ **可视化预览**：生成配置后立即可视化预览，而非纯文本

### 6.3 明确技术边界

**修正建议**：
- ✅ **核心引擎**：阶段、行动、资源、牌堆、胜负（完全信息）
- ⚠️ **扩展层**：隐藏信息、叛徒、Legacy（Phase 5+）
- ❌ **不支持**：实时谈判、社交推理、复杂空间移动（版图类游戏）

---

## 七、总结：批判性结论

### 7.1 核心问题

1. **原文档过于乐观**：
   - 「80%+ 游戏可覆盖」缺乏数据支撑
   - LLM 生成 YAML 的可靠性被高估
   - 隐藏信息模拟的难度被低估

2. **技术难点被简化**：
   - YAML 配置的表达力有限，复杂逻辑需要脚本
   - 多智能体合作模拟需要专门研究
   - 配置文件会爆炸式增长，维护困难

3. **用户体验未充分讨论**：
   - 学习曲线、调试困难、LLM 对话的局限性

### 7.2 修正后的可行性

| 维度 | 修正后评估 |
|------|------------|
| **技术可行性** | ⚠️ **中等**：核心功能可行，但扩展功能难度高 |
| **市场可行性** | ⚠️ **中等**：需要明确目标用户和差异化 |
| **用户体验** | ⚠️ **中等**：需要图形化工具和模板库 |
| **商业可行性** | ⚠️ **中等**：需要明确商业模式和价值主张 |

### 7.3 最终建议

1. **缩小范围**：先做「半合作 RPG」类游戏，而非「所有 RPG 桌游」
2. **降低 LLM 期望**：LLM 作为「辅助工具」而非「全自动生成器」
3. **分阶段验证**：每个阶段都要验证「用户是否真的需要这个功能」
4. **承认局限性**：不是所有游戏都适合，不是所有机制都能用配置表达

**结论**：项目**可行，但需要大幅缩小范围和降低期望**。建议先做 MVP（功德轮回 + 简单回测），验证用户需求后再扩展。
